#2. Load Outcome data
outcome_df <- read.csv(paste0(data_dir,outcome_file),stringsAsFactors = F)
outcome_df <- outcome_df[match(feature_df[,"STUDY_PATIENT_ID"],outcome_df[,"STUDY_PATIENT_ID"]),] #  #reorder outcome to match ID
#3.Get pts IDs who did not died in hosp
surviors_IDs1 <- outcome_df[which(outcome_df[,"Death_inHOSP"] == 0),"STUDY_PATIENT_ID"]
#4.Get pts IDs who did not died in the range of hosp start to 120 after hosp discharge
surviors_IDs2 <- outcome_df[which(outcome_df[,"Death_HOSPStartTo120"] == 0),"STUDY_PATIENT_ID"]
updated_outcome_df <- outcome_df[outcome_df$STUDY_PATIENT_ID %in% c(surviors_IDs1),]
updated_outcome_df <- outcome_df[outcome_df$STUDY_PATIENT_ID %in% c(surviors_IDs1,surviors_IDs2),]
setdiff(surviors_IDs2,surviors_IDs1)
setdiff(surviors_IDs1,surviors_IDs2)
#3.Get pts IDs who died in hosp
died_inHOSP_ID <- outcome_df[which(outcome_df[,"Death_inHOSP"] == 1),"STUDY_PATIENT_ID"]
#4.Get pts IDs who did not died in the range of hosp start to 120 after hosp discharge
died_inHOSPstartTo120_ID <- outcome_df[which(outcome_df[,"Death_HOSPStartTo120"] == 1),"STUDY_PATIENT_ID"]
updated_outcome_df <- outcome_df[-outcome_df$STUDY_PATIENT_ID %in% c(died_inHOSP_ID,died_inHOSPstartTo120_ID),]
feature_df <- read.csv(paste0(data_dir,feature_file),stringsAsFactors = F)
#2. Load Outcome data
outcome_df <- read.csv(paste0(data_dir,outcome_file),stringsAsFactors = F)
outcome_df <- outcome_df[match(feature_df[,"STUDY_PATIENT_ID"],outcome_df[,"STUDY_PATIENT_ID"]),] #  #reorder outcome to match ID
#3.Get pts IDs who died in hosp
died_inHOSP_ID <- outcome_df[which(outcome_df[,"Death_inHOSP"] == 1),"STUDY_PATIENT_ID"]
#4.Get pts IDs who did not died in the range of hosp start to 120 after hosp discharge
died_inHOSPstartTo120_ID <- outcome_df[which(outcome_df[,"Death_HOSPStartTo120"] == 1),"STUDY_PATIENT_ID"]
updated_outcome_df <- outcome_df[-which(outcome_df$STUDY_PATIENT_ID %in% c(died_inHOSP_ID,died_inHOSPstartTo120_ID)),]
which(updated_outcome_df$Death_inHOSP==1)
which(updated_outcome_df$Death_HOSPStartTo120)
which(updated_outcome_df$Death_HOSPStartTo120==1)
updated_feature_df <- feature_df[-which(feature_df$STUDY_PATIENT_ID %in% c(died_inHOSP_ID,died_inHOSPstartTo120_ID)),]
identical(outcome_df[,"STUDY_PATIENT_ID"],feature_df[,"STUDY_PATIENT_ID"])==T
source("TAKI_Ultility.R")
library(rms)
library(PredictABEL)
library(pROC) #can also use this one for delong's methods
library(Rmisc)
library(caret)
#this script do 10 folds CV on UK data
#1. for each fold , down sampling 10 time, each instance get 10 predicted results
#2. compute confidence interval for performance metrics for each fold with each sampling index
#Data dir
data_dir <- "/Volumes/LJL_ExtPro/Data/AKI_Data/TAKI_Data_Extracted/uky/Model_Feature_Outcome/"
#out dir
out_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/CV_performance/"
#feature file and outcome file names
outcome_file <- "All_outcome.csv"
#1.All_Feature_imputed_normed.csv
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features2 <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/SelectedClinicalFeature14Vars/")
#Outcome column name
outcome_colname <- "MAKE_HOSP120_Drop50"
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
model_data <- model_data[,c(selected_features2,outcome_colname)]
#Only get data for pts who survived in hosp
table(model_data$MAKE_HOSP120_Drop50)
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
#Only get data for survirors
construct_model_data_func_survirors <- function(data_dir,feature_file,outcome_file,outcome_colname){
#1.Load feature data
feature_df <- read.csv(paste0(data_dir,feature_file),stringsAsFactors = F)
#2. Load Outcome data
outcome_df <- read.csv(paste0(data_dir,outcome_file),stringsAsFactors = F)
outcome_df <- outcome_df[match(feature_df[,"STUDY_PATIENT_ID"],outcome_df[,"STUDY_PATIENT_ID"]),] #  #reorder outcome to match ID
#3.Get pts IDs who died in hosp
died_inHOSP_ID <- outcome_df[which(outcome_df[,"Death_inHOSP"] == 1),"STUDY_PATIENT_ID"]
#4.Get pts IDs who did  died in the range of hosp start to 120 after hosp discharge
died_inHOSPstartTo120_ID <- outcome_df[which(outcome_df[,"Death_HOSPStartTo120"] == 1),"STUDY_PATIENT_ID"]
#5.exclude ID did in hosp and died from hosp start to 120
updated_outcome_df <- outcome_df[-which(outcome_df$STUDY_PATIENT_ID %in% c(died_inHOSP_ID,died_inHOSPstartTo120_ID)),]
updated_feature_df <- feature_df[-which(feature_df$STUDY_PATIENT_ID %in% c(died_inHOSP_ID,died_inHOSPstartTo120_ID)),]
#3.Check if IDs order are matched, if so process
if(identical(updated_outcome_df[,"STUDY_PATIENT_ID"],updated_feature_df[,"STUDY_PATIENT_ID"])==T){
#4.Add outcome to feature data as train data
model_data <- updated_feature_df
model_data[,outcome_colname] <- updated_outcome_df[,outcome_colname]
#5.Add ID as row name, and remove ID col
rownames(model_data) <- model_data[,"STUDY_PATIENT_ID"] #add ID as
model_data <- model_data[,-1]
#6.Recode label as Y and N, because caret package does not accept 1 or 0
model_data <- code_Label_YN_func(model_data,outcome_colname)
}else{
model_data <- NULL
print("Feature and Outcome IDs does not match")
}
return(model_data)
}
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
model_data <- model_data[,c(selected_features2,outcome_colname)]
#Only get data for pts who survived in hosp
table(model_data$MAKE_HOSP120_Drop50)
colnames(model_data)
#1.All_Feature_imputed_normed.csv
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features2 <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/SelectedClinicalFeature14Vars/")
#Outcome column name
outcome_colname <- "MAKE_HOSP120_Drop50"
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
model_data <- model_data[,c(selected_features2,outcome_colname)]
table(model_data$MAKE_HOSP120_Drop50)
colnames(model_data)
#2.CV
upsample_flag <- 0
N_sampling <- 10
NFolds <- 10
model_name_list <- c("SVM","RF","LogReg","XGB")
for (m in 1:length(model_name_list)){
model_name <- model_name_list[m]
#CV
cv_res <- cv2_func(model_data,outcome_colname,model_name,upsample_flag,N_sampling,NFolds,svmkernel = "svmLinear2") #svmPoly, svmLinear,svmLinear2
final_pred <- cv_res[[1]]
write.csv(final_pred, paste0(outdir1,"Prediction_", model_name, ".csv"),row.names = F)
#compute avg performance
final_importance_matrix <- cv_res[[2]]
feature_indexes<- which(colnames(model_data) != outcome_colname)
features <- colnames(model_data)[feature_indexes]
avg_importance_matrix <- compute_avg_importance(final_importance_matrix,features,model_name)
write.csv(avg_importance_matrix, paste0(outdir1,"Importance_AVG_", model_name, ".csv"),row.names = F)
#Compute perforamnce for each fold with each sampling
eachfold_eachSample_perf_tb <- compute_performance_TrainCV_func(N_sampling,NFolds,final_pred)
write.csv(eachfold_eachSample_perf_tb, paste0(outdir1,"Performance_PerFoldPerSample_", model_name, ".csv"),row.names = F)
#get CI and mean perforamnce
CI_perf_tb <- perf_Mean_CI_func(eachfold_eachSample_perf_tb[,3:14])
write.csv(CI_perf_tb, paste0(outdir1,"Performance_AVG_CI_", model_name, ".csv"),row.names = T)
}
#######################################################################################
######                MAKE with drop50 Prediction   6                      ############
##'@NOTE: only do prediction for survirors (pts who  did not die from hosp start to hosp 120)
#feature file: 1. KDIGO.csv,
#Outcome file: All_outcome.csv
#######################################################################################
#1.All_Feature_imputed_normed.csv
feature_file <- c("All_MAX_KDIGO_ICUD0toD3_normed.csv")
#Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/KDIGO/")
#Outcome column name
outcome_colname <- "MAKE_HOSP120_Drop50"
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
table(model_data$MAKE_HOSP120_Drop50)
colnames(model_data)
#2.CV
upsample_flag <- 0
N_sampling <- 10
NFolds <- 10
model_name_list <- c("SVM","RF","LogReg","XGB")
for (m in 1:length(model_name_list)){
model_name <- model_name_list[m]
#CV
cv_res <- cv2_func(model_data,outcome_colname,model_name,upsample_flag,N_sampling,NFolds,svmkernel = "svmLinear2")
final_pred <- cv_res[[1]]
write.csv(final_pred, paste0(outdir1,"Prediction_", model_name, ".csv"),row.names = F)
#compute avg performance
final_importance_matrix <- cv_res[[2]]
feature_indexes<- which(colnames(model_data) != outcome_colname)
features <- colnames(model_data)[feature_indexes]
avg_importance_matrix <- compute_avg_importance(final_importance_matrix,features,model_name)
write.csv(avg_importance_matrix, paste0(outdir1,"Importance_AVG_", model_name, ".csv"),row.names = F)
#Compute perforamnce for each fold with each sampling
eachfold_eachSample_perf_tb <- compute_performance_TrainCV_func(N_sampling,NFolds,final_pred)
write.csv(eachfold_eachSample_perf_tb, paste0(outdir1,"Performance_PerFoldPerSample_", model_name, ".csv"),row.names = F)
#get CI and mean perforamnce
CI_perf_tb <- perf_Mean_CI_func(eachfold_eachSample_perf_tb[,3:14])
write.csv(CI_perf_tb, paste0(outdir1,"Performance_AVG_CI_", model_name, ".csv"),row.names = T)
}
source("TAKI_Ultility.R")
#this script use entire UK data, and validation on utsw data (Use down sampling and bootstrapping for CI )
main_func <-function(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list,n_tress_RF=500,svmkernel = 'svmLinear2',random_perc=0.8){
for (m in 1:length(method_list)){
model_name <- method_list[m]
#External validation training with downsampled UK data 10 times and validate on UTSW data
res <- external_validation_func(train_data,Validation_data,outcome_colname,model_name,upsample_flag,N_sampling,n_tress_RF,svmkernel,random_perc)
final_pred <- res[[1]]
write.csv(final_pred, paste0(outdir1,"Prediction_", model_name, ".csv"),row.names = F)
#compute avg performance
final_importance_matrix <- res[[2]]
features <- colnames(train_data)[which(colnames(train_data) != outcome_colname)]
avg_importance_matrix <- compute_avg_importance(final_importance_matrix,features,model_name)
write.csv(avg_importance_matrix, paste0(outdir1,"Importance_AVG_", model_name, ".csv"),row.names = F)
#Compute perforamnce for each sampling
eachSample_perf_tb <- compute_performance_ExternalValidation_func(N_sampling,final_pred)
write.csv(eachSample_perf_tb, paste0(outdir1,"Performance_PerFoldPerSample_", model_name, ".csv"),row.names = F)
#get CI and mean perforamnce
eachSample_perf_tb[which(is.na(eachSample_perf_tb)==T,arr.ind = T)] <- 0 #basicaly prediction only 1 class in these samples
CI_perf_tb <- perf_Mean_CI_func(eachSample_perf_tb[,2:13])
write.csv(CI_perf_tb, paste0(outdir1,"Performance_AVG_CI_", model_name, ".csv"),row.names = T)
}
}
#Data dir
UK_data_dir <- "/Volumes/LJL_ExtPro/Data/AKI_Data/TAKI_Data_Extracted/uky/Model_Feature_Outcome/"
UTSW_data_dir <- "/Volumes/LJL_ExtPro/Data/AKI_Data/TAKI_Data_Extracted/utsw/Model_Feature_Outcome/"
#out dir
out_dir <- "//Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/ExternalV_performance/"
#1.All_Feature_imputed_normed.csv
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features2 <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/SelectedClinicalFeature14Vars/")
#Outcome column name
outcome_colname <- "MAKE_HOSP120_Drop50"
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
model_data <- model_data[,c(selected_features2,outcome_colname)]
table(model_data$MAKE_HOSP120_Drop50)
colnames(model_data)
#1.Feature file
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "MAKE_HOSP120_Drop50"
#######################################################################################
######                MAKE with drop50 Prediction   4                      ############
#'@NOTE: only do prediction for survirors (pts who  did not die from hosp start to hosp 120)
#feature file: Selected Features 14 vars (prediction3 without onRRT)
#Outcome file: All_outcome.csv
#######################################################################################
#1.All_Feature_imputed_normed.csv
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features2 <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/SelectedClinicalFeature14Vars/")
#Outcome column name
outcome_colname <- "MAKE_HOSP120_Drop50"
#1.Get model data
model_data <- construct_model_data_func_survirors(data_dir,feature_file,outcome_file,outcome_colname)
#######################################################################################
######                MAKE with drop50 Prediction   4                      ############
#'@NOTE: only do prediction for survirors (pts who  did not die from hosp start to hosp 120)
#feature file: Selected Features 14 vars (prediction3 without onRRT)
#Outcome file: All_outcome.csv
#######################################################################################
#1.Feature file
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "MAKE_HOSP120_Drop50"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"make120_drop50/SelectedClinicalFeature14Vars/")
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/SelectedClinicalFeature14Vars/")
#1.Get model data
train_data <- construct_model_data_func(UK_data_dir,feature_file,outcome_file,outcome_colname)
#1.Get model data
train_data <- construct_model_data_func_survirors(UK_data_dir,feature_file,outcome_file,outcome_colname)
train_data <- train_data[,c(selected_features,outcome_colname)]
Validation_data <- construct_model_data_func_survirors(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- Validation_data[,c(selected_features,outcome_colname)]
table(train_data$MAKE_HOSP120_Drop50) #4972 2382
table(Validation_data$MAKE_HOSP120_Drop50) #1659  574
#1.Feature file
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW",
"Admit_sCr","Sodium_D1_LOW")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "MAKE_HOSP120_Drop50"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/SelectedClinicalFeature14Vars/")
#1.Get model data
train_data <- construct_model_data_func_survirors(UK_data_dir,feature_file,outcome_file,outcome_colname)
train_data <- train_data[,c(selected_features,outcome_colname)]
Validation_data <- construct_model_data_func_survirors(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- Validation_data[,c(selected_features,outcome_colname)]
table(train_data$MAKE_HOSP120_Drop50) #4972  423
table(Validation_data$MAKE_HOSP120_Drop50) #1659  205
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list)
#1.Feature file
feature_file <- c("All_MAX_KDIGO_ICUD0toD3_normed.csv")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "MAKE_HOSP120_Drop50"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"Surviors_make120_drop50/KDIGO/")
#1.Get model data
train_data <- construct_model_data_func_survirors(UK_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- construct_model_data_func_survirors(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
table(train_data$MAKE_HOSP120_Drop50) #4972 2382
table(Validation_data$MAKE_HOSP120_Drop50) #1659  574
#1.Get model data
train_data <- construct_model_data_func_survirors(UK_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- construct_model_data_func_survirors(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
table(train_data$MAKE_HOSP120_Drop50) #4972  423
table(Validation_data$MAKE_HOSP120_Drop50) #1659  205
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list,n_tress_RF=500,svmkernel = 'svmLinear2',random_perc=0.8)
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"CV_performance/Surviors_make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
source("TAKI_Ultility.R")
get_allmethods_performance <- function(folder_name,file_names,feature_set_name){
file_dir <- paste0(folder_name,feature_set_name,"/",file_names)
perfs_list <- list(NA)
for (i in 1:length(file_dir)){
curr_file <- file_dir[i]
curr_method_name <- gsub(paste0(folder_name,feature_set_name,"/Performance_AVG_CI_|.csv"),"",curr_file)
curr_perf <- read.csv(curr_file ,stringsAsFactors = F)
colnames(curr_perf) <- c("Metrics",paste0(feature_set_name,"_",curr_method_name,"_Mean_95CI"))
perfs_list[[i]] <- curr_perf
}
perfs <- do.call(cbind,perfs_list)
perfs <- perfs[,-c(3,5,7)] #remove duplicated "metric" col
return(perfs)
}
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"CV_performance/Surviors_make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
AllClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"AllClinicalFeature")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
colnames(all_perfs)
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
folder_name
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
#######################################################################################
#'@ADDITIONAL_PERFORMANCE
##### external Validation  MAKE drop 50 for survivors
#######################################################################################
rm() #clear all vairbales
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"ExternalV_performance/Surviors_make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
folder_name
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
source("TAKI_Ultility.R")
compute_avg_pred_risk_and_risk_category <- function(cohort_name,outcome_name,perf_dir,method_name,featureset_folder){
# perf_dir <- UTSW_MAKE_dir
# cohort_name <- "UTSW"
# outcome_name <- "MAKE"
# method_name <- "RF"
#1. Load pred table
pred_df <- read.csv(paste0(perf_dir, featureset_folder, "/Prediction_",method_name,".csv"),stringsAsFactors = F)
#2.Compute avg pred risk
avg_risk <- get_avg_pred_func(pred_df)
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
write.csv(risk_count1,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory1_",method_name,".csv"))
risk_category2 <- c(0.2,0.5)
risk_count2 <- count_risk_category(avg_risk,risk_category2)
write.csv(risk_count2,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory2_",method_name,".csv"))
risk_category3 <- c(0.1,0.3,0.5)
risk_count3 <- count_risk_category(avg_risk,risk_category3)
write.csv(risk_count3,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory3_",method_name,".csv"))
}
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature14Vars"
outcome_name <- "MAKE"
#1. UK
UK_MAKE_dir <- paste0(proj_dir,"CV_performance/Surviors_make120_drop50/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_MAKE_dir,method_name,featureset_folder)
#2.UTSW
UTSW_MAKE_dir <- paste0(proj_dir,"ExternalV_performance/Surviors_make120_drop50/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_MAKE_dir,method_name,featureset_folder)
###############################################################
#'@ADDITONAL MAKE for survivors
#3. For MAKE, compare models with IDI, NRI
###############################################################
#1.UK
perf_dir <- paste0(proj_dir,"CV_performance/Surviors_make120_drop50/")
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
###############################################################
#'@ADDITONAL MAKE for survivors
#3. For MAKE, compare models with IDI, NRI
###############################################################
#1.UK
perf_dir <- paste0(proj_dir,"CV_performance/Surviors_make120_drop50/")
baseline_model_file  <- "/KDIGO/Prediction_RF.csv"
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
write.csv(reclass_res,paste0(perf_dir,"UK_SelectedClinicalFeature14Vars_MAKE_ReclassResults_RF.csv"))
#UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/Surviors_make120_drop50/")
baseline_model_file  <- "/KDIGO/Prediction_RF.csv"
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
write.csv(reclass_res,paste0(perf_dir,"UTSW_SelectedClinicalFeature14Varsl_MAKE_ReclassResults_RF.csv"))
