outcome_name <- "Mortality"
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature2"
#1. UK
UK_mortality_dir <- paste0(proj_dir,"CV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_mortality_dir,method_name,featureset_folder)
#2.UTSW
UTSW_mortality_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_mortality_dir,method_name,featureset_folder)
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature"
outcome_name <- "MAKE"
#1. UK
UK_MAKE_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UK__",outcome_name,UK_MAKE_dir,method_name,featureset_folder)
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature"
outcome_name <- "MAKE"
#1. UK
UK_MAKE_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_MAKE_dir,method_name,featureset_folder)
#2.UTSW
UTSW_MAKE_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_MAKE_dir,method_name,featureset_folder)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
#2. UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature2/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature2vsSOFA_",colnames(reclass_res2)[2])
comb_res <- cbind(reclass_res1,reclass_res2)
View(comb_res)
write.csv(comb_res,paste0(perf_dir,"UTSW_SelectedClinicalFeature2_Mortality_ReclassResults_RF.csv"))
perf_dir
#UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
baseline_model_file  <- "/KDIGO/Prediction_RF.csv"
comprison_model_file1 <- "/SelectedClinicalFeature/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalvsKDIGO_",colnames(reclass_res)[2])
write.csv(reclass_res,paste0(perf_dir,"UTSW_SelectedClinical_MAKE_ReclassResults_RF.csv"))
#######################################################################################
##### 3. External Validation Mortality performance
#######################################################################################
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
folder_name <- paste0(perf_dir,"ExternalV_performance/mortality/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
source("TAKI_Ultility.R")
get_allmethods_performance <- function(folder_name,file_names,feature_set_name){
file_dir <- paste0(folder_name,feature_set_name,"/",file_names)
perfs_list <- list(NA)
for (i in 1:length(file_dir)){
curr_file <- file_dir[i]
curr_method_name <- gsub(paste0(folder_name,feature_set_name,"/Performance_AVG_CI_|.csv"),"",curr_file)
curr_perf <- read.csv(curr_file ,stringsAsFactors = F)
colnames(curr_perf) <- c("Metrics",paste0(feature_set_name,"_",curr_method_name,"_Mean_95CI"))
perfs_list[[i]] <- curr_perf
}
perfs <- do.call(cbind,perfs_list)
perfs <- perfs[,-c(3,5,7)] #remove duplicated "metric" col
return(perfs)
}
#######################################################################################
##### 3. External Validation Mortality performance
#######################################################################################
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
folder_name <- paste0(perf_dir,"ExternalV_performance/mortality/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
SelectedClinicalFeature_perfs2 <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature2")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
APACHE_perfs
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
SelectedClinicalFeature_perfs2 <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature2")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
SelectedClinicalFeature_perfs2 <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature2")
all_perfs <- cbind(SOFA_perfs,APACHE_perfs,SelectedClinicalFeature_perfs,SelectedClinicalFeature_perfs2,AllClinicalFeature_perfs)
all_perfs <- cbind(SOFA_perfs,APACHE_perfs,SelectedClinicalFeature_perfs2,AllClinicalFeature_perfs)
all_perfs <- cbind(SOFA_perfs,APACHE_perfs,SelectedClinicalFeature_perfs2)
colnames(all_perfs)
all_perfs <- all_perfs[-c(6,11)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs)))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "SOFA"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
folder_name
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
#######################################################################################
##### 4.External Validation  MAKE drop 50
#######################################################################################
rm() #clear all vairbales
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
folder_name <- paste0(perf_dir,"CV_performance/make120_drop50/")
folder_name <- paste0(perf_dir,"ExternalV_performance/make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
SelectedClinicalFeature_perfs2 <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature2")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs,SelectedClinicalFeature_perfs2,AllClinicalFeature_perfs)
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6,11)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
folder_name
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
source("TAKI_Ultility.R")
#this script use entire UK data, and validation on utsw data (Use down sampling and bootstrapping for CI )
main_func <-function(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list,n_tress_RF=500,svmkernel = 'svmLinear2',random_perc=0.8){
for (m in 1:length(method_list)){
model_name <- method_list[m]
#External validation training with downsampled UK data 10 times and validate on UTSW data
res <- external_validation_func(train_data,Validation_data,outcome_colname,model_name,upsample_flag,N_sampling,n_tress_RF,svmkernel,random_perc)
final_pred <- res[[1]]
write.csv(final_pred, paste0(outdir1,"Prediction_", model_name, ".csv"),row.names = F)
#compute avg performance
final_importance_matrix <- res[[2]]
features <- colnames(train_data)[which(colnames(train_data) != outcome_colname)]
avg_importance_matrix <- compute_avg_importance(final_importance_matrix,features,model_name)
write.csv(avg_importance_matrix, paste0(outdir1,"Importance_AVG_", model_name, ".csv"),row.names = F)
#Compute perforamnce for each sampling
eachSample_perf_tb <- compute_performance_ExternalValidation_func(N_sampling,final_pred)
write.csv(eachSample_perf_tb, paste0(outdir1,"Performance_PerFoldPerSample_", model_name, ".csv"),row.names = F)
#get CI and mean perforamnce
eachSample_perf_tb[which(is.na(eachSample_perf_tb)==T,arr.ind = T)] <- 0 #basicaly prediction only 1 class in these samples
CI_perf_tb <- perf_Mean_CI_func(eachSample_perf_tb[,2:13])
write.csv(CI_perf_tb, paste0(outdir1,"Performance_AVG_CI_", model_name, ".csv"),row.names = T)
}
}
#Data dir
UK_data_dir <- "/Volumes/LJL_ExtPro/Data/AKI_Data/TAKI_Data_Extracted/uky/Model_Feature_Outcome/"
UTSW_data_dir <- "/Volumes/LJL_ExtPro/Data/AKI_Data/TAKI_Data_Extracted/utsw/Model_Feature_Outcome/"
#out dir
out_dir <- "//Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/ExternalV_performance/"
#######################################################################################
######                           Mortality Prediction   1                  ############
#feature file: SOFA.csv,
#Outcome file: All_outcome.csv
#######################################################################################
#1.Feature file
feature_file <- "All_SOFA_TOTAL_normed.csv"
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "Death_inHOSP"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"mortality/SOFA/")
#1.Get model data
train_data <- construct_model_data_func(UK_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- construct_model_data_func(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
table(train_data$Death_inHOSP) #5742 1612
table(Validation_data$Death_inHOSP) #2011  222
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list)
#######################################################################################
######                           Mortality Prediction   2                  ############
#feature file: APACHE.csv,
#Outcome file: All_outcome.csv
#######################################################################################
#1.Feature file
feature_file <- "All_APACHE_TOTAL_normed.csv"
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "Death_inHOSP"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"mortality/APACHE/")
#1.Get model data
train_data <- construct_model_data_func(UK_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- construct_model_data_func(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
table(train_data$Death_inHOSP) #5742 1612
table(Validation_data$Death_inHOSP) #2011  222
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list)
##1.Feature file
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features <- c("UrineOutput_D0toD3" , "Vasopressor_ICUD0toD3","FI02_D1_HIGH","Platelets_D1_LOW","AGE",
"BUN_D0toD3_HIGH","HR_D1_HIGH","LAST_KDIGO_ICU_D0toD3","PH_D1_LOW","Bilirubin_D1_HIGH",
"MAX_KDIGO_ICU_D0toD3","ECMO_ICUD0toD3","Hours_inICUD0toD3", "Temperature_D1_LOW", "Temperature_D1_HIGH")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "Death_inHOSP"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"mortality/SelectedClinicalFeature2/")
#1.Get model data
train_data <- construct_model_data_func(UK_data_dir,feature_file,outcome_file,outcome_colname)
train_data <- train_data[,c(selected_features,outcome_colname)]
Validation_data <- construct_model_data_func(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- Validation_data[,c(selected_features,outcome_colname)]
table(train_data$Death_inHOSP) #5742 1612
table(Validation_data$Death_inHOSP) #2011  222
colnames(train_data)
colnames(Validation_data)
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list)
#######################################################################################
######                MAKE with drop50 Prediction   1                      ############
#feature file: 1. KDIGO.csv,
#Outcome file: All_outcome.csv
#######################################################################################
#1.Feature file
feature_file <- c("All_MAX_KDIGO_ICUD0toD3_normed.csv")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "MAKE_HOSP120_Drop50"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"make120_drop50/KDIGO/")
#1.Get model data
train_data <- construct_model_data_func(UK_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- construct_model_data_func(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
table(train_data$MAKE_HOSP120_Drop50) #4972 2382
table(Validation_data$MAKE_HOSP120_Drop50) #1659  574
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list,n_tress_RF=500,svmkernel = 'svmLinear2',random_perc=0.8)
#1.Feature file
feature_file <- c("All_Feature_imputed_normed.csv")
selected_features <- c("LAST_KDIGO_ICU_D0toD3","UrineOutput_D0toD3","MAX_KDIGO_ICU_D0toD3","Bilirubin_D1_HIGH",
"AGE","BUN_D0toD3_HIGH","Hemoglobin_D1_LOW","Platelets_D1_LOW","FI02_D1_HIGH",
"Vasopressor_ICUD0toD3","HR_D1_HIGH","PH_D1_LOW")
#2.Outcome column name
outcome_file <- "All_outcome.csv"
outcome_colname <- "MAKE_HOSP120_Drop50"
#3.Outdir for mortality
outdir1 <- paste0(out_dir,"make120_drop50/SelectedClinicalFeature/")
#1.Get model data
train_data <- construct_model_data_func(UK_data_dir,feature_file,outcome_file,outcome_colname)
train_data <- train_data[,c(selected_features,outcome_colname)]
Validation_data <- construct_model_data_func(UTSW_data_dir,feature_file,outcome_file,outcome_colname)
Validation_data <- Validation_data[,c(selected_features,outcome_colname)]
table(train_data$MAKE_HOSP120_Drop50) #4972 2382
table(Validation_data$MAKE_HOSP120_Drop50) #1659  574
#2.For each method, do boostraps 10 times on entire UK data, and valdition on UTSW data
upsample_flag <- 3 #random sample 0.8 of train data with replacement for bootstrapping and then down sample for training
N_sampling <- 10
method_list <- c("SVM","RF","LogReg","XGB")
main_func(train_data,Validation_data,outcome_colname,upsample_flag,N_sampling,outdir1,method_list)
source("TAKI_Ultility.R")
get_allmethods_performance <- function(folder_name,file_names,feature_set_name){
file_dir <- paste0(folder_name,feature_set_name,"/",file_names)
perfs_list <- list(NA)
for (i in 1:length(file_dir)){
curr_file <- file_dir[i]
curr_method_name <- gsub(paste0(folder_name,feature_set_name,"/Performance_AVG_CI_|.csv"),"",curr_file)
curr_perf <- read.csv(curr_file ,stringsAsFactors = F)
colnames(curr_perf) <- c("Metrics",paste0(feature_set_name,"_",curr_method_name,"_Mean_95CI"))
perfs_list[[i]] <- curr_perf
}
perfs <- do.call(cbind,perfs_list)
perfs <- perfs[,-c(3,5,7)] #remove duplicated "metric" col
return(perfs)
}
#######################################################################################
##### 3. External Validation Mortality performance
#######################################################################################
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
folder_name <- paste0(perf_dir,"ExternalV_performance/mortality/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
SelectedClinicalFeature_perfs2 <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature2")
all_perfs <- cbind(SOFA_perfs,APACHE_perfs,SelectedClinicalFeature_perfs2)
all_perfs <- all_perfs[-c(6,11)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs)))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "SOFA"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
folder_name
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
folder_name <- paste0(perf_dir,"ExternalV_performance/make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6,11)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
source("TAKI_Ultility.R")
compute_avg_pred_risk_and_risk_category <- function(cohort_name,outcome_name,perf_dir,method_name,featureset_folder){
# perf_dir <- UK_mortality_dir
# cohort_name <- "UK"
# outcome_name <- "Mortality"
#1. Load pred table
pred_df <- read.csv(paste0(perf_dir, featureset_folder, "/Prediction_",method_name,".csv"),stringsAsFactors = F)
#2.Compute avg pred risk
avg_risk <- get_avg_pred_func(pred_df)
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
write.csv(risk_count1,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory1_",method_name,".csv"))
risk_category2 <- c(0.2,0.5)
risk_count2 <- count_risk_category(avg_risk,risk_category2)
write.csv(risk_count2,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory2_",method_name,".csv"))
risk_category3 <- c(0.1,0.3,0.5)
risk_count3 <- count_risk_category(avg_risk,risk_category3)
write.csv(risk_count3,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory3_",method_name,".csv"))
}
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
outcome_name <- "Mortality"
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature2"
#2.UTSW
UTSW_mortality_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_mortality_dir,method_name,featureset_folder)
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature"
outcome_name <- "MAKE"
#2.UTSW
UTSW_MAKE_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_MAKE_dir,method_name,featureset_folder)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
#2. UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature2/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature2vsSOFA_",colnames(reclass_res2)[2])
comb_res <- cbind(reclass_res1,reclass_res2)
write.csv(comb_res,paste0(perf_dir,"UTSW_SelectedClinicalFeature2_Mortality_ReclassResults_RF.csv"))
#UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
baseline_model_file  <- "/KDIGO/Prediction_RF.csv"
comprison_model_file1 <- "/SelectedClinicalFeature/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalvsKDIGO_",colnames(reclass_res)[2])
write.csv(reclass_res,paste0(perf_dir,"UTSW_SelectedClinical_MAKE_ReclassResults_RF.csv"))
