#All_perfs_df: all performance dataframe
#bl_feature: baseline feature name
#bl_method: baseline method name
#pred_file_folder: prediction file folder
# all_perfs_df <- all_perfs
# bl_feature <- "SOFA"
# bl_method  <- "LogReg"
#Get Baseline AUC
bl_auc_colidxes <- which(grepl(paste0(bl_feature,"_",bl_method),colnames(all_perfs_df))== T)
bl_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
bl_auc    <-  all_perfs_df[bl_auc_rowidxes,bl_auc_colidxes]
bl_auc    <- as.numeric(unlist(strsplit(bl_auc,split = "(",fixed = T))[1])
#Comput AUC diff for each other feature set and methods
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs_df)))
colnames(AUC_diff) <- colnames(all_perfs_df)
AUC_diff$Metrics[1] <-  paste0("AUC_Diff_",bl_feature,"_", bl_method)
AUC_diff$Metrics[2] <-  paste0("AUC_Diff_Pvalue_",bl_feature,"_", bl_method)
for (i in 2:ncol(AUC_diff)){ #for each feature set
#Get current column name and feature and method
curr_col <- colnames(AUC_diff)[i]
curr_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method  <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_feature != bl_feature){
cur_auc_colidxes <- which(grepl(paste0(curr_feature,"_",curr_method),colnames(all_perfs_df))== T)
cur_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
cur_auc          <-  all_perfs_df[cur_auc_rowidxes,cur_auc_colidxes]
cur_auc          <- as.numeric(unlist(strsplit(cur_auc,split = "(",fixed = T))[[1]])
#Compute AUC diff
AUC_diff[1,i] <- round(cur_auc - bl_auc,2)
#Compute P value
bl_pred_file   <- paste0(bl_feature,"/Prediction_",bl_method,".csv")
curr_pred_file <- paste0(curr_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(pred_file_folder,bl_pred_file,curr_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
return(AUC_diff)
}
#this function compute the AUC difference between
#baseline feature and all other feature with corresponding methods
compute_AUC_diff_func2 <- function(all_perfs_df, bl_feature, pred_file_folder){
#Input:
#All_perfs_df: all performance dataframe
#bl_feature: baseline feature name
#pred_file_folder: prediction file folder
#Comput AUC diff for each other feature set and methods
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs_df)))
colnames(AUC_diff) <- colnames(all_perfs_df)
AUC_diff$Metrics[1] <-  paste0("AUC_Diff_",bl_feature,"_CorrespondMethod")
AUC_diff$Metrics[2] <-  paste0("AUC_Diff_Pvalue_",bl_feature,"_CorrespondMethod")
for (i in 2:ncol(AUC_diff)){ #for each feature set
#Get current column name and feature and method
curr_col <- colnames(AUC_diff)[i]
curr_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method  <- unlist(strsplit(curr_col,split = "_"))[2]
#Get Baseline AUC
bl_auc_colidxes <- which(grepl(paste0(bl_feature,"_",curr_method),colnames(all_perfs_df))== T)
bl_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
bl_auc    <-  all_perfs_df[bl_auc_rowidxes,bl_auc_colidxes]
bl_auc    <- as.numeric(unlist(strsplit(bl_auc,split = "(",fixed = T))[1])
if (curr_feature != bl_feature){
cur_auc_colidxes <- which(grepl(paste0(curr_feature,"_",curr_method),colnames(all_perfs_df))== T)
cur_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
cur_auc          <-  all_perfs_df[cur_auc_rowidxes,cur_auc_colidxes]
cur_auc          <- as.numeric(unlist(strsplit(cur_auc,split = "(",fixed = T))[[1]])
#Compute AUC diff
AUC_diff[1,i] <- round(cur_auc - bl_auc,2)
#Compute P value
bl_pred_file   <- paste0(bl_feature,"/Prediction_",curr_method,".csv")
curr_pred_file <- paste0(curr_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(pred_file_folder,bl_pred_file,curr_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
return(AUC_diff)
}
rm() #clear all vairbales
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"ExternalV_performance/Surviors_make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6)]
#reorder rows
reorder_names <- c("AUC" , "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
all_perfs <- all_perfs[match(reorder_names,all_perfs$Metrics),]
#2.1 For each featuresets and each method, compare with baseline (KDIGO + Logreg) AUC diff
AUC_diff_LR <- compute_AUC_diff_func1(all_perfs,"KDIGO","LogReg",folder_name)
#2.2. For each featuresets, compare with baseline (KDIGO + corresponding ML method) AUC diff
AUC_diff_Corr <- compute_AUC_diff_func2(all_perfs,"KDIGO",folder_name)
AUC_diff <- rbind(AUC_diff_LR, AUC_diff_Corr) #Combine the two
Final_all_perfs <- rbind(all_perfs,AUC_diff)
AUC_diff_Corr$Metrics
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff_KDIGO_CorrespondMethod"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue_KDIGO_CorrespondMethod"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
#'@TODO:
#'#'@check
identical(AUC_diff,AUC_diff_Corr)
source("TAKI_Ultility.R")
get_allmethods_performance <- function(folder_name,file_names,feature_set_name){
file_dir <- paste0(folder_name,feature_set_name,"/",file_names)
perfs_list <- list(NA)
for (i in 1:length(file_dir)){
curr_file <- file_dir[i]
curr_method_name <- gsub(paste0(folder_name,feature_set_name,"/Performance_AVG_CI_|.csv"),"",curr_file)
curr_perf <- read.csv(curr_file ,stringsAsFactors = F)
colnames(curr_perf) <- c("Metrics",paste0(feature_set_name,"_",curr_method_name,"_Mean_95CI"))
perfs_list[[i]] <- curr_perf
}
perfs <- do.call(cbind,perfs_list)
perfs <- perfs[,-c(3,5,7)] #remove duplicated "metric" col
return(perfs)
}
#this function compute the AUC difference between
#baseline feature + LogReg and all other feature with other methods
compute_AUC_diff_func1 <- function(all_perfs_df, bl_feature, bl_method,pred_file_folder){
#Input:
#All_perfs_df: all performance dataframe
#bl_feature: baseline feature name
#bl_method: baseline method name
#pred_file_folder: prediction file folder
# all_perfs_df <- all_perfs
# bl_feature <- "SOFA"
# bl_method  <- "LogReg"
#Get Baseline AUC
bl_auc_colidxes <- which(grepl(paste0(bl_feature,"_",bl_method),colnames(all_perfs_df))== T)
bl_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
bl_auc    <-  all_perfs_df[bl_auc_rowidxes,bl_auc_colidxes]
bl_auc    <- as.numeric(unlist(strsplit(bl_auc,split = "(",fixed = T))[1])
#Comput AUC diff for each other feature set and methods
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs_df)))
colnames(AUC_diff) <- colnames(all_perfs_df)
AUC_diff$Metrics[1] <-  paste0("AUC_Diff_",bl_feature,"_", bl_method)
AUC_diff$Metrics[2] <-  paste0("AUC_Diff_Pvalue_",bl_feature,"_", bl_method)
for (i in 2:ncol(AUC_diff)){ #for each feature set
#Get current column name and feature and method
curr_col <- colnames(AUC_diff)[i]
curr_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method  <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_feature != bl_feature){
cur_auc_colidxes <- which(grepl(paste0(curr_feature,"_",curr_method),colnames(all_perfs_df))== T)
cur_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
cur_auc          <-  all_perfs_df[cur_auc_rowidxes,cur_auc_colidxes]
cur_auc          <- as.numeric(unlist(strsplit(cur_auc,split = "(",fixed = T))[[1]])
#Compute AUC diff
AUC_diff[1,i] <- round(cur_auc - bl_auc,2)
#Compute P value
bl_pred_file   <- paste0(bl_feature,"/Prediction_",bl_method,".csv")
curr_pred_file <- paste0(curr_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(pred_file_folder,bl_pred_file,curr_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
return(AUC_diff)
}
#this function compute the AUC difference between
#baseline feature and all other feature with corresponding methods
compute_AUC_diff_func2 <- function(all_perfs_df, bl_feature, pred_file_folder){
#Input:
#All_perfs_df: all performance dataframe
#bl_feature: baseline feature name
#pred_file_folder: prediction file folder
#Comput AUC diff for each other feature set and methods
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs_df)))
colnames(AUC_diff) <- colnames(all_perfs_df)
AUC_diff$Metrics[1] <-  paste0("AUC_Diff_",bl_feature,"_CorrespondMethod")
AUC_diff$Metrics[2] <-  paste0("AUC_Diff_Pvalue_",bl_feature,"_CorrespondMethod")
for (i in 2:ncol(AUC_diff)){ #for each feature set
#Get current column name and feature and method
curr_col <- colnames(AUC_diff)[i]
curr_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method  <- unlist(strsplit(curr_col,split = "_"))[2]
#Get Baseline AUC
bl_auc_colidxes <- which(grepl(paste0(bl_feature,"_",curr_method),colnames(all_perfs_df))== T)
bl_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
bl_auc    <-  all_perfs_df[bl_auc_rowidxes,bl_auc_colidxes]
bl_auc    <- as.numeric(unlist(strsplit(bl_auc,split = "(",fixed = T))[1])
if (curr_feature != bl_feature){
cur_auc_colidxes <- which(grepl(paste0(curr_feature,"_",curr_method),colnames(all_perfs_df))== T)
cur_auc_rowidxes <- which(all_perfs_df$Metrics == "AUC")
cur_auc          <-  all_perfs_df[cur_auc_rowidxes,cur_auc_colidxes]
cur_auc          <- as.numeric(unlist(strsplit(cur_auc,split = "(",fixed = T))[[1]])
#Compute AUC diff
AUC_diff[1,i] <- round(cur_auc - bl_auc,2)
#Compute P value
bl_pred_file   <- paste0(bl_feature,"/Prediction_",curr_method,".csv")
curr_pred_file <- paste0(curr_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(pred_file_folder,bl_pred_file,curr_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
return(AUC_diff)
}
#######################################################################################
#'@ADDITIONAL_PERFORMANCE
##### 5.Cross Validation  MAKE drop 50 for survivors
#######################################################################################
rm() #clear all vairbales
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"CV_performance/Surviors_make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6)]
#reorder rows
reorder_names <- c("AUC" , "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
all_perfs <- all_perfs[match(reorder_names,all_perfs$Metrics),]
#2.1 For each featuresets and each method, compare with baseline (KDIGO + Logreg) AUC diff
AUC_diff_LR <- compute_AUC_diff_func1(all_perfs,"KDIGO","LogReg",folder_name)
#2.2. For each featuresets, compare with baseline (KDIGO + corresponding ML method) AUC diff
AUC_diff_Corr <- compute_AUC_diff_func2(all_perfs,"KDIGO",folder_name)
AUC_diff <- rbind(AUC_diff_LR, AUC_diff_Corr) #Combine the two
Final_all_perfs <- rbind(all_perfs,AUC_diff)
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
#######################################################################################
#'@ADDITIONAL_PERFORMANCE
##### external Validation  MAKE drop 50 for survivors
#######################################################################################
rm() #clear all vairbales
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"ExternalV_performance/Surviors_make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs  <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6)]
#reorder rows
reorder_names <- c("AUC" , "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
all_perfs <- all_perfs[match(reorder_names,all_perfs$Metrics),]
#2.1 For each featuresets and each method, compare with baseline (KDIGO + Logreg) AUC diff
AUC_diff_LR <- compute_AUC_diff_func1(all_perfs,"KDIGO","LogReg",folder_name)
#2.2. For each featuresets, compare with baseline (KDIGO + corresponding ML method) AUC diff
AUC_diff_Corr <- compute_AUC_diff_func2(all_perfs,"KDIGO",folder_name)
AUC_diff <- rbind(AUC_diff_LR, AUC_diff_Corr) #Combine the two
Final_all_perfs <- rbind(all_perfs,AUC_diff)
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
#compare APAHCE and SOFA
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
#compare clinical model and SOFA
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
View(comb_res)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/SOFA/Prediction_",baseline_method,".csv")
comprison_model_file1 <- paste0("/APACHE/Prediction_",baseline_method,".csv")
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
#compare APAHCE and SOFA
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
#compare clinical model and SOFA
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
outfile <- paste0(perf_dir,"UK_SelectedClinicalFeature15Vars_Mortality_ReclassResults_",baseline_method,".csv")
outfile
write.csv(comb_res,outfile)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
baseline_method <- "RF"
baseline_model_file  <- paste0("/SOFA/Prediction_",baseline_method,".csv")
comprison_model_file1 <- paste0("/APACHE/Prediction_",baseline_method,".csv")
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
#compare APAHCE and SOFA
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
#compare clinical model and SOFA
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/SOFA/Prediction_",baseline_method,".csv")
comprison_model_file1 <- paste0("/APACHE/Prediction_",baseline_method,".csv")
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
outfile <- paste0(perf_dir,"UTSW_SelectedClinicalFeature15Vars_Mortality_ReclassResults_",baseline_method,".csv")
write.csv(comb_res,outfile)
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_method <- "RF"
baseline_model_file  <- paste0("/SOFA/Prediction_",baseline_method,".csv")
comprison_model_file1 <- paste0("/APACHE/Prediction_",baseline_method,".csv")
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
View(comb_res)
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/SOFA/Prediction_",baseline_method,".csv")
comprison_model_file1 <- paste0("/APACHE/Prediction_",baseline_method,".csv")
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
outfile <- paste0(perf_dir,"UTSW_SelectedClinicalFeature15Vars_Mortality_ReclassResults_",baseline_method,".csv")
write.csv(comb_res,outfile)
baseline_model_file
comprison_model_file1
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/SOFA/Prediction_",baseline_method,".csv")
comprison_model_file1 <- paste0("/APACHE/Prediction_",baseline_method,".csv")
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
#compare clinical model and APAHCE
reclass_res3 <- compute_IDI_NRI_func(perf_dir,comprison_model_file1,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res3)[2] <- paste0("SelectedClinicalFeature15VarsvsAPACHE_",colnames(reclass_res3)[2])
comb_res <- cbind(reclass_res1,reclass_res2,reclass_res3)
outfile <- paste0(perf_dir,"UTSW_SelectedClinicalFeature15Vars_Mortality_ReclassResults_",baseline_method,".csv")
write.csv(comb_res,outfile)
View(comb_res)
perf_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
baseline_method <- "RF"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
View(reclass_res)
perf_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
outfile <- paste0(perf_dir,"UK_SelectedClinicalFeature14Vars_MAKE_ReclassResults_",baseline_method,".csv")
write.csv(reclass_res,outfile)
View(reclass_res)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
#1.UK
perf_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
outfile <- paste0(perf_dir,"UK_SelectedClinicalFeature14Vars_MAKE_ReclassResults_",baseline_method,".csv")
write.csv(reclass_res,outfile)
#UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
outfile <- paste0(perf_dir,"UTSW_SelectedClinicalFeature14Vars_MAKE_ReclassResults_",baseline_method,".csv")
write.csv(reclass_res,outfile)
perf_dir <- paste0(proj_dir,"CV_performance/Surviors_make120_drop50/")
baseline_method <- "RF"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
perf_dir <- paste0(proj_dir,"ExternalV_performance/Surviors_make120_drop50/")
baseline_method <- "RF"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
#1.UK
perf_dir <- paste0(proj_dir,"CV_performance/Surviors_make120_drop50/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
outfile <- paste0(perf_dir,"UK_SelectedClinicalFeature14Vars_MAKE_ReclassResults_",baseline_method,".csv")
write.csv(reclass_res,outfile)
#UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/Surviors_make120_drop50/")
baseline_method <- "LogReg"
baseline_model_file  <- paste0("/KDIGO/Prediction_",baseline_method,".csv")
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
outfile <- paste0(perf_dir,"UTSW_SelectedClinicalFeature14Vars_MAKE_ReclassResults_",baseline_method,".csv")
write.csv(reclass_res,outfile)
