outcome_colname <- "died_inp"
#feature file list
feature_file_list <- c("SOFA_SUM_norm.csv","APACHE_SUM_norm.csv",
"updated_clinical_model_mortality_norm.csv",
"updated_clinical_model_mortality_wTrajectory_norm.csv",
"Mortality_selected_features_norm.csv")
f <- 1
#Feature file :
feature_file <- feature_file_list[[f]]    #UK
UTSW_feature_file <- feature_file_list[[f]]   #UTSW
#out file name
outfile_pname <- gsub("_make|_mortality|_norm.csv","",feature_file)
outfile_pname
####################################################################################################
##### UK data and outcome
####################################################################################################
#Load feature data
feature_df <- read.csv(paste0(UK_dir,feature_file),stringsAsFactors = F)
#Load Outcome data
outcome_df <- read.csv(paste0(UK_dir,outcome_file),stringsAsFactors = F)
table(updated_outcome_df$MAKE)
table(outcome_df$MAKE)
outcome_colname
colnames(outcome_df)
#Add outcome to feature data
feature_df[,outcome_colname] <- outcome_df[match(outcome_df[,"STUDY_PATIENT_ID"],feature_df[,"STUDY_PATIENT_ID"]),outcome_colname]
rownames(feature_df)
rownames(feature_df) <- feature_df$STUDY_PATIENT_ID
feature_df <- feature_df[,-1] #remove ID col
View(feature_df)
####################################################################################################
##### UTSW validation data and outcome
####################################################################################################
#Load feature data
UTSW_feature_df <- read.csv(paste0(UTSW_dir,UTSW_feature_file),stringsAsFactors = F)
#Load Outcome data
UTSW_outcome_df <- read.csv(paste0(UTSW_dir,UTSW_outcome_file),stringsAsFactors = F)
####################################################################################################
##### UTSW validation data and outcome
####################################################################################################
#Load feature data
UTSW_feature_df <- read.csv(paste0(UTSW_dir,UTSW_feature_file),stringsAsFactors = F)
feature_file_list
source("TAKI_Ultility.R")
main <- function(outdir,outdir2,UK_dir,UTSW_dir, feature_file,outcome_file,outcome_colname,outfile_pname){
####################################################################################################
##### UK data and outcome
####################################################################################################
#Load feature data
feature_df <- read.csv(paste0(UK_dir,feature_file),stringsAsFactors = F)
#Load Outcome data
outcome_df <- read.csv(paste0(UK_dir,outcome_file),stringsAsFactors = F)
table(outcome_df$MAKE)
#Add outcome to feature data
feature_df[,outcome_colname] <- outcome_df[match(outcome_df[,"STUDY_PATIENT_ID"],feature_df[,"STUDY_PATIENT_ID"]),outcome_colname]
rownames(feature_df) <- feature_df$STUDY_PATIENT_ID
feature_df <- feature_df[,-1] #remove ID col
####################################################################################################
##### UTSW validation data and outcome
####################################################################################################
#Load feature data
UTSW_feature_df <- read.csv(paste0(UTSW_dir,UTSW_feature_file),stringsAsFactors = F)
#Load Outcome data
UTSW_outcome_df <- read.csv(paste0(UTSW_dir,UTSW_outcome_file),stringsAsFactors = F)
updated_UTSW_outcome_df <- UTSW_outcome_df[which(UTSW_outcome_df[,"STUDY_PATIENT_ID"] %in% UTSW_feature_df[,"STUDY_PATIENT_ID"]),]
#Add outcome to feature data
UTSW_feature_df[,UTSW_outcome_colname] <- updated_UTSW_outcome_df[match(updated_UTSW_outcome_df[,"STUDY_PATIENT_ID"],UTSW_feature_df[,"STUDY_PATIENT_ID"]),UTSW_outcome_colname]
rownames(UTSW_feature_df) <- UTSW_feature_df$STUDY_PATIENT_ID
UTSW_feature_df <- UTSW_feature_df[,-1] #remove ID col
#change utsw outcome name to uky outcome name
colnames(UTSW_feature_df)[which(colnames(UTSW_feature_df) == UTSW_outcome_colname)] <- outcome_colname
#analysis Id
analysis_df <- feature_df
validation_df <- UTSW_feature_df
#################################################
####### 10-fold cv Training And prediction #######
#################################################
upsample_flag <- 0 #ds
N_sampling <- 10
# SVM
pred_table_SVM <- cv_func(analysis_df,outcome_colname,"SVM",validation_df,upsample_flag,N_sampling)
UKY_perd_table <- pred_table_SVM[[1]]
UTSW_perd_table <- pred_table_SVM[[2]]
final_mportance_matrix <- pred_table_SVM[[3]]
write.csv(UKY_perd_table,paste0(outdir,outfile_pname,"_pred_table_SVM_UKY.csv"))
write.csv(UTSW_perd_table,paste0(outdir,outfile_pname,"_pred_table_SVM_UTSW.csv"))
write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_SVM.csv"))
# RF
pred_table_RF <- cv_func(analysis_df,outcome_colname,"RF",validation_df,upsample_flag,N_sampling)
UKY_perd_table_RF <- pred_table_RF[[1]]
UTSW_perd_table_RF <- pred_table_RF[[2]]
final_mportance_matrix <- pred_table_RF[[3]]
write.csv(UKY_perd_table_RF,paste0(outdir,outfile_pname,"_pred_table_RF_UKY.csv"))
write.csv(UTSW_perd_table_RF,paste0(outdir,outfile_pname,"_pred_table_RF_UTSW.csv"))
write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_RF.csv"))
# Logreg
pred_table_Logreg <- cv_func(analysis_df,outcome_colname,"LogReg",validation_df,upsample_flag,N_sampling)
UKY_perd_table_Logreg <- pred_table_Logreg[[1]]
UTSW_perd_table_Logreg <- pred_table_Logreg[[2]]
final_mportance_matrix <- pred_table_Logreg[[3]]
write.csv(UKY_perd_table_Logreg,paste0(outdir,outfile_pname,"_pred_table_Logreg_UKY.csv"))
write.csv(UTSW_perd_table_Logreg,paste0(outdir,outfile_pname,"_pred_table_Logreg_UTSW.csv"))
write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_Logreg.csv"))
# Xgboost
pred_table_xgb<- cv_func(analysis_df,outcome_colname,"XGB",validation_df,upsample_flag,N_sampling)
UKY_perd_table_xgb <- pred_table_xgb[[1]]
UTSW_perd_table_xgb <- pred_table_xgb[[2]]
final_mportance_matrix <- pred_table_xgb[[3]]
write.csv(UKY_perd_table_xgb,paste0(outdir,outfile_pname,"_pred_table_Xgb_UKY.csv"))
write.csv(UTSW_perd_table_xgb,paste0(outdir,outfile_pname,"_pred_table_Xgb_UTSW.csv"))
write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_XGB.csv"))
# #Only do top feature model if the dataset have >25 features, in our study, it is the clinical  + trajectory feature model
# if (ncol(analysis_df) > 26){ #inlcuding outcome column
#     # Xgboost topfeature
#     pred_table_xbg_top<- cv_func(analysis_df,outcome_colname,"XGB_TOP",validation_df,upsample_flag,N_sampling)
#     UKY_perd_table_xbg_top <- pred_table_xbg_top[[1]]
#     UTSW_perd_table_xbg_top <- pred_table_xbg_top[[2]]
#     final_mportance_matrix <- pred_table_xbg_top[[3]]
#     write.csv(UKY_perd_table_xbg_top,paste0(outdir,outfile_pname,"_pred_table_xbg_top_UKY.csv"))
#     write.csv(UTSW_perd_table_xbg_top,paste0(outdir,outfile_pname,"_pred_table_xbg_top_UTSW.csv"))
#     write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_XGBTOP.csv"))
#
#
#     # SVM topfeature
#     pred_table_SVM_top<- cv_func(analysis_df,outcome_colname,"SVM_TOP",validation_df,upsample_flag,N_sampling)
#     UKY_perd_table_SVM_top <- pred_table_SVM_top[[1]]
#     UTSW_perd_table_SVM_top <- pred_table_SVM_top[[2]]
#     final_mportance_matrix <- pred_table_SVM_top[[3]]
#     write.csv(UKY_perd_table_SVM_top,paste0(outdir,outfile_pname,"_pred_table_SVM_top_UKY.csv"))
#     write.csv(UTSW_perd_table_SVM_top,paste0(outdir,outfile_pname,"_pred_table_SVM_top_UTSW.csv"))
#     write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_SVMTOP.csv"))
#
#     # RF topfeature
#     pred_table_RF_top<- cv_func(analysis_df,outcome_colname,"RF_TOP",validation_df,upsample_flag,N_sampling)
#     UKY_perd_table_RF_top <- pred_table_RF_top[[1]]
#     UTSW_perd_table_RF_top <- pred_table_RF_top[[2]]
#     final_mportance_matrix <- pred_table_RF_top[[3]]
#     write.csv(UKY_perd_table_RF_top,paste0(outdir,outfile_pname,"_pred_table_RF_top_UKY.csv"))
#     write.csv(UTSW_perd_table_RF_top,paste0(outdir,outfile_pname,"_pred_table_RF_top_UTSW.csv"))
#     write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_RFTOP.csv"))
#
#     # Logreg topfeature
#     pred_table_LogReg_top<- cv_func(analysis_df,outcome_colname,"LogReg_TOP",validation_df,upsample_flag,N_sampling)
#     UKY_perd_table_LogReg_top <- pred_table_LogReg_top[[1]]
#     UTSW_perd_table_LogReg_top <- pred_table_LogReg_top[[2]]
#     final_mportance_matrix <- pred_table_LogReg_top[[3]]
#     write.csv(UKY_perd_table_LogReg_top,paste0(outdir,outfile_pname,"_pred_table_LogReg_top_UKY.csv"))
#     write.csv(UTSW_perd_table_LogReg_top,paste0(outdir,outfile_pname,"_pred_table_LogReg_top_UTSW.csv"))
#     write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_LogRegTOP.csv"))
# }
}
#######################################################################################
######                           mortality Prediction                      ############
#1. SOFA_SUM_norm.csv
#2. APACHE_SUM_norm.csv
#3. clinical_model_mortality_norm.csv
#4. clinical_model_mortality_wTrajectory_norm.csv
#5. Mortality_selected_features_norm.csv
#######################################################################################
#User input
data_dir <- "/Volumes/LJL_ExtPro/Data/AKI_Data/Taylors_Data/TAKI_Feature/Updated_features/"
#data dir
UK_dir <- paste0(data_dir, "uky/")
UTSW_dir <- paste0(data_dir, "utsw/")
#out dir
out_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0427/"
outdir <- paste0(out_dir,"mortality/")
outdir2 <- paste0(out_dir,"mortality_importance/")
#Outcome file :
outcome_file <- "updated_outcome.csv"
outcome_colname <- "died_inp"
#feature file list
feature_file_list <- c("SOFA_SUM_norm.csv","APACHE_SUM_norm.csv",
"updated_clinical_model_mortality_norm.csv",
"updated_clinical_model_mortality_wTrajectory_norm.csv",
"Mortality_selected_features_norm.csv")
####################################################################################################
##### UK data and outcome
####################################################################################################
#Load feature data
feature_df <- read.csv(paste0(UK_dir,feature_file),stringsAsFactors = F)
f <- 1
#Feature file :
feature_file <- feature_file_list[[f]]
#out file name
outfile_pname <- gsub("_make|_mortality|_norm.csv","",feature_file)
outfile_pname
####################################################################################################
##### UK data and outcome
####################################################################################################
#Load feature data
feature_df <- read.csv(paste0(UK_dir,feature_file),stringsAsFactors = F)
#Load Outcome data
outcome_df <- read.csv(paste0(UK_dir,outcome_file),stringsAsFactors = F)
table(outcome_df$MAKE)
#Add outcome to feature data
feature_df[,outcome_colname] <- outcome_df[match(outcome_df[,"STUDY_PATIENT_ID"],feature_df[,"STUDY_PATIENT_ID"]),outcome_colname]
rownames(feature_df) <- feature_df$STUDY_PATIENT_ID
feature_df <- feature_df[,-1] #remove ID col
####################################################################################################
##### UTSW validation data and outcome
####################################################################################################
#Load feature data
UTSW_feature_df <- read.csv(paste0(UTSW_dir,UTSW_feature_file),stringsAsFactors = F)
####################################################################################################
##### UTSW validation data and outcome
####################################################################################################
#Load feature data
UTSW_feature_df <- read.csv(paste0(UTSW_dir,feature_file),stringsAsFactors = F)
#Load Outcome data
UTSW_outcome_df <- read.csv(paste0(UTSW_dir,outcome_file),stringsAsFactors = F)
#Add outcome to feature data
UTSW_feature_df[,UTSW_outcome_colname] <- UTSW_outcome_df[match(UTSW_outcome_df[,"STUDY_PATIENT_ID"],UTSW_feature_df[,"STUDY_PATIENT_ID"]),UTSW_outcome_colname]
#Add outcome to feature data
UTSW_feature_df[,outcome_colname] <- UTSW_outcome_df[match(UTSW_outcome_df[,"STUDY_PATIENT_ID"],UTSW_feature_df[,"STUDY_PATIENT_ID"]),outcome_colname]
rownames(UTSW_feature_df) <- UTSW_feature_df$STUDY_PATIENT_ID
UTSW_feature_df <- UTSW_feature_df[,-1] #remove ID col
#analysis df
analysis_df <- feature_df
validation_df <- UTSW_feature_df
#################################################
####### 10-fold cv Training And prediction #######
#################################################
upsample_flag <- 0 #ds
N_sampling <- 10
# SVM
pred_table_SVM <- cv_func(analysis_df,outcome_colname,"SVM",validation_df,upsample_flag,N_sampling)
UKY_perd_table <- pred_table_SVM[[1]]
UTSW_perd_table <- pred_table_SVM[[2]]
View(UKY_perd_table)
write.csv(UKY_perd_table,paste0(outdir,outfile_pname,"_pred_table_SVM_UKY.csv"))
write.csv(UTSW_perd_table,paste0(outdir,outfile_pname,"_pred_table_SVM_UTSW.csv"))
write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_SVM.csv"))
final_mportance_matrix <- pred_table_SVM[[3]]
write.csv(final_mportance_matrix,paste0(outdir2,outcome_colname,"_",outfile_pname,"_importance_matrix_SVM.csv"))
source('~/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/TAKI_Code/4_Classification2.R', echo=TRUE)
library(rms)
library(PredictABEL)
library(pROC) #can also use this one for delong's methods
library(Rmisc)
#compute calibration slope and Intercept
compute_calibration_func <-function(perf_table){
#perf_table <- curr_table
#compute calibration Intercept and slope and plot
pred_p <-   perf_table[,"pred_prob"]
acutal <- as.numeric(as.vector(perf_table[,"Label"]))
res = val.prob(pred_p,acutal,m=100, cex=.5,pl=FALSE)
calib_res <- res[c("Intercept","Slope")]
#Note: This is what val.prb actually doing
#glm(acutal ~ log(pred_p/(1-pred_p)),family="binomial")
#another way mentioned in Taylor's email:
#Slope: Mean Model Output for All With Positive Outcome / Mean Model Output for All With Negative Outcome>
#Intercept: Mean model output for all patients with a negative result for the outcome.
all_pos_output <- perf_table[which(perf_table[,"Label"]==1),"pred_prob"]
all_neg_output <- perf_table[which(perf_table[,"Label"]==0),"pred_prob"]
possible_slope <- mean(all_pos_output)/mean(all_neg_output)
possible_Intercept <- mean(all_neg_output)
return(list(calib_res,possible_Intercept,possible_slope))
}
#Compute AUC, ACC, Precision, Sensitivity and...
compute_performance_func <- function(prediction_table){
#prediction_table <- curr_v_tab
prediction_table[,"pred_class"] <- as.factor(prediction_table[,"pred_class"])
prediction_table[,"Label"] <- as.factor(prediction_table[,"Label"])
pred_prob <- prediction_table[,"pred_prob"]
pred_label <- prediction_table[,"pred_class"]
actual_label <- prediction_table[,"Label"]
#ROC AUC
AUC_res <- roc(actual_label, pred_prob,direction = "<",ci =T, auc= T, quiet=TRUE) # control:0, case:1
AUC <- as.numeric(AUC_res$auc)
#By default, this function uses 2000 bootstraps to calculate a 95% confidence interval.
AUC_95CI <- c(as.numeric(ci.auc(actual_label,pred_prob,direction = "<", quiet=TRUE))[1],as.numeric(ci.auc(actual_label,pred_prob,direction = "<"))[3])
cm <- confusionMatrix(pred_label, actual_label, positive = "1", dnn = c("Prediction", "Reference"), mode="everything")
ACC <- cm$overall[1]
#NOte: Use Exact binomial testto compute CI for accuracy, first value is #number of sucess, #number of failure
##Note: Following uses binom.test(c(4883,1283),conf.level = 0.95)
ACC_95CI <- cm$overall[c("AccuracyLower","AccuracyUpper")] #
Precision <- cm$byClass["Precision"]
Sensitivity <- cm$byClass["Sensitivity"]
Specificity  <- cm$byClass["Specificity"]
PPV <- cm$byClass["Pos Pred Value"]
NPV <- cm$byClass["Neg Pred Value"]
F1_Score <- cm$byClass["F1"]
perf_vec <- c(AUC,AUC_95CI,ACC,ACC_95CI,Precision,Sensitivity,Specificity,PPV,NPV,F1_Score)
return(perf_vec)
}
#Compute report mean and CI for all folds
perf_Mean_CI_Folds_func <-function(Fold_perf_table){
#Fold_perf_table <- EachFold_perf_table
mean_CI_perf <- as.data.frame(matrix(NA,nrow = ncol(Fold_perf_table),ncol = 1))
colnames(mean_CI_perf) <- "Mean_(95CI)"
rownames(mean_CI_perf) <-  colnames(Fold_perf_table)
for (j in 1:ncol(Fold_perf_table)){
curr_CI <- CI(Fold_perf_table[,j], ci=0.95)
curr_CI <- as.numeric(round(curr_CI,2))
mean_CI_perf[j,1] <- paste0(curr_CI[2], "(",curr_CI[3],"-",curr_CI[1],")")
}
return(mean_CI_perf)
}
compute_AUC_diff_func <-function(perf_dir,baseline_model_file,comprison_model_file1){
baseline_df <- read.csv(paste0(perf_dir,"/",baseline_model_file),stringsAsFactors = F)
comp_df <- read.csv(paste0(perf_dir,"/",comprison_model_file1),stringsAsFactors = F)
#Combine comparison models
model_comp_df <- cbind.data.frame(baseline_df[,"Label"],
baseline_df[,"pred_prob"],
comp_df[,"pred_prob"])
colnames(model_comp_df) <- c("Label","pred_prob_bl","pred_prob1")
#AUC difference
roc_bl <- roc(model_comp_df$Label, model_comp_df$pred_prob_bl)
roc_1 <- roc(model_comp_df$Label, model_comp_df$pred_prob1)
test_res <- roc.test(roc_bl,roc_1,method = "delong")
pval <- test_res$p.value
return(pval)
}
## compute reclassification measures
compute_IDI_NRI_func <-function(perf_dir,baseline_model_file,comprison_model_file1){
baseline_df <- read.csv(paste0(perf_dir,"/",baseline_model_file),stringsAsFactors = F)
comp_df <- read.csv(paste0(perf_dir,"/",comprison_model_file1),stringsAsFactors = F)
#Combine comparison models
model_comp_df <- cbind.data.frame(baseline_df[,"Label"],
baseline_df[,"pred_prob"],
comp_df[,"pred_prob"])
colnames(model_comp_df) <- c("Label","pred_prob_bl","pred_prob1")
predRisk1 <- model_comp_df$pred_prob_bl
predRisk2 <- model_comp_df$pred_prob1
cutoff <- c(0,.5,1)
#NRI = P(up|event)−P(down|event) + P(down|nonevent)−P(up|nonevent)
res <- reclassification(data=model_comp_df, cOutcome=1, predrisk1=predRisk1, predrisk2=predRisk2, cutoff)
return(res)
}
#read perforamnce table
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0427/mortality/"
#perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0427/make/"
outdir <- perf_dir
all_perf_files <- list.files(perf_dir,full.names = T)
all_perf_files
perf_table_list <- lapply(all_perf_files, read.csv, stringsAsFactors = F)
########
File_perf <- list(NA)
for (i in 1:length(perf_table_list)){ #for each file
curr_table <- perf_table_list[[i]]
curr_file <- gsub(perf_dir,"",all_perf_files[[i]])
EachFold_perf_table <- as.data.frame(matrix(NA, nrow =10 ,ncol = 12))
rownames(EachFold_perf_table) <- paste0("Fold",seq(1,10))
colnames(EachFold_perf_table) <-c("AUC","Accuracy","Precision","Sensitivity","Specificity","PPV","NPV",
"Calibration_Intercept","Calibration_Slope",
"Taylor_Calibration_Intercept","Taylor_Calibration_Slope","F1")
for (s in 1:10){ #for each fold
curr_v_tab <- curr_table[which(curr_table[,"TestFold"] == paste0("Fold",s)),]
#calibration
calib_res <- compute_calibration_func(curr_v_tab)
my_calib_res <- calib_res[[1]]
taylors_res <- c(calib_res[[2]],calib_res[[3]])
EachFold_perf_table[s,"Calibration_Intercept"] <- my_calib_res[1]
EachFold_perf_table[s,"Calibration_Slope"] <- my_calib_res[2]
EachFold_perf_table[s,"Taylor_Calibration_Intercept"] <- taylors_res[1]
EachFold_perf_table[s,"Taylor_Calibration_Slope"] <- taylors_res[2]
#Other perforamnce:
other_res <- compute_performance_func(curr_v_tab)
EachFold_perf_table[s,"AUC"] <- as.numeric(other_res[1])
EachFold_perf_table[s,"Accuracy"] <- other_res["Accuracy"]
EachFold_perf_table[s,"Precision"] <- other_res["Precision"]
EachFold_perf_table[s,"Sensitivity"] <- other_res["Sensitivity"]
EachFold_perf_table[s,"Specificity"] <- other_res["Specificity"]
EachFold_perf_table[s,"PPV"] <- other_res["Pos Pred Value"]
EachFold_perf_table[s,"NPV"] <- other_res["Neg Pred Value"]
EachFold_perf_table[s,"F1"] <- other_res["F1"]
}
curr_File_perf <- perf_Mean_CI_Folds_func(EachFold_perf_table)
colnames(curr_File_perf) <- gsub("/|.csv|_pred_table","",curr_file)
File_perf[[i]] <- curr_File_perf
}
Final_perf <- do.call(cbind,File_perf)
write.csv(Final_perf, paste0(outdir , "Final_ALLperf.csv"))
library(rms)
library(PredictABEL)
library(pROC) #can also use this one for delong's methods
library(Rmisc)
#compute calibration slope and Intercept
compute_calibration_func <-function(perf_table){
#perf_table <- curr_table
#compute calibration Intercept and slope and plot
pred_p <-   perf_table[,"pred_prob"]
acutal <- as.numeric(as.vector(perf_table[,"Label"]))
res = val.prob(pred_p,acutal,m=100, cex=.5,pl=FALSE)
calib_res <- res[c("Intercept","Slope")]
#Note: This is what val.prb actually doing
#glm(acutal ~ log(pred_p/(1-pred_p)),family="binomial")
#another way mentioned in Taylor's email:
#Slope: Mean Model Output for All With Positive Outcome / Mean Model Output for All With Negative Outcome>
#Intercept: Mean model output for all patients with a negative result for the outcome.
all_pos_output <- perf_table[which(perf_table[,"Label"]==1),"pred_prob"]
all_neg_output <- perf_table[which(perf_table[,"Label"]==0),"pred_prob"]
possible_slope <- mean(all_pos_output)/mean(all_neg_output)
possible_Intercept <- mean(all_neg_output)
return(list(calib_res,possible_Intercept,possible_slope))
}
#Compute AUC, ACC, Precision, Sensitivity and...
compute_performance_func <- function(prediction_table){
#prediction_table <- curr_v_tab
prediction_table[,"pred_class"] <- as.factor(prediction_table[,"pred_class"])
prediction_table[,"Label"] <- as.factor(prediction_table[,"Label"])
pred_prob <- prediction_table[,"pred_prob"]
pred_label <- prediction_table[,"pred_class"]
actual_label <- prediction_table[,"Label"]
#ROC AUC
AUC_res <- roc(actual_label, pred_prob,direction = "<",ci =T, auc= T, quiet=TRUE) # control:0, case:1
AUC <- as.numeric(AUC_res$auc)
#By default, this function uses 2000 bootstraps to calculate a 95% confidence interval.
AUC_95CI <- c(as.numeric(ci.auc(actual_label,pred_prob,direction = "<", quiet=TRUE))[1],as.numeric(ci.auc(actual_label,pred_prob,direction = "<"))[3])
cm <- confusionMatrix(pred_label, actual_label, positive = "1", dnn = c("Prediction", "Reference"), mode="everything")
ACC <- cm$overall[1]
#NOte: Use Exact binomial testto compute CI for accuracy, first value is #number of sucess, #number of failure
##Note: Following uses binom.test(c(4883,1283),conf.level = 0.95)
ACC_95CI <- cm$overall[c("AccuracyLower","AccuracyUpper")] #
Precision <- cm$byClass["Precision"]
Sensitivity <- cm$byClass["Sensitivity"]
Specificity  <- cm$byClass["Specificity"]
PPV <- cm$byClass["Pos Pred Value"]
NPV <- cm$byClass["Neg Pred Value"]
F1_Score <- cm$byClass["F1"]
perf_vec <- c(AUC,AUC_95CI,ACC,ACC_95CI,Precision,Sensitivity,Specificity,PPV,NPV,F1_Score)
return(perf_vec)
}
#Compute report mean and CI for all folds
perf_Mean_CI_Folds_func <-function(Fold_perf_table){
#Fold_perf_table <- EachFold_perf_table
mean_CI_perf <- as.data.frame(matrix(NA,nrow = ncol(Fold_perf_table),ncol = 1))
colnames(mean_CI_perf) <- "Mean_(95CI)"
rownames(mean_CI_perf) <-  colnames(Fold_perf_table)
for (j in 1:ncol(Fold_perf_table)){
curr_CI <- CI(Fold_perf_table[,j], ci=0.95)
curr_CI <- as.numeric(round(curr_CI,2))
mean_CI_perf[j,1] <- paste0(curr_CI[2], "(",curr_CI[3],"-",curr_CI[1],")")
}
return(mean_CI_perf)
}
compute_AUC_diff_func <-function(perf_dir,baseline_model_file,comprison_model_file1){
baseline_df <- read.csv(paste0(perf_dir,"/",baseline_model_file),stringsAsFactors = F)
comp_df <- read.csv(paste0(perf_dir,"/",comprison_model_file1),stringsAsFactors = F)
#Combine comparison models
model_comp_df <- cbind.data.frame(baseline_df[,"Label"],
baseline_df[,"pred_prob"],
comp_df[,"pred_prob"])
colnames(model_comp_df) <- c("Label","pred_prob_bl","pred_prob1")
#AUC difference
roc_bl <- roc(model_comp_df$Label, model_comp_df$pred_prob_bl)
roc_1 <- roc(model_comp_df$Label, model_comp_df$pred_prob1)
test_res <- roc.test(roc_bl,roc_1,method = "delong")
pval <- test_res$p.value
return(pval)
}
## compute reclassification measures
compute_IDI_NRI_func <-function(perf_dir,baseline_model_file,comprison_model_file1){
baseline_df <- read.csv(paste0(perf_dir,"/",baseline_model_file),stringsAsFactors = F)
comp_df <- read.csv(paste0(perf_dir,"/",comprison_model_file1),stringsAsFactors = F)
#Combine comparison models
model_comp_df <- cbind.data.frame(baseline_df[,"Label"],
baseline_df[,"pred_prob"],
comp_df[,"pred_prob"])
colnames(model_comp_df) <- c("Label","pred_prob_bl","pred_prob1")
predRisk1 <- model_comp_df$pred_prob_bl
predRisk2 <- model_comp_df$pred_prob1
cutoff <- c(0,.5,1)
#NRI = P(up|event)−P(down|event) + P(down|nonevent)−P(up|nonevent)
res <- reclassification(data=model_comp_df, cOutcome=1, predrisk1=predRisk1, predrisk2=predRisk2, cutoff)
return(res)
}
#read perforamnce table
#perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0427/mortality/"
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0427/make/"
outdir <- perf_dir
all_perf_files <- list.files(perf_dir,full.names = T)
#all_perf_files <- all_perf_files[-which(grepl("Final_ALLperf.csv",all_perf_files)==T)]
perf_table_list <- lapply(all_perf_files, read.csv, stringsAsFactors = F)
########
File_perf <- list(NA)
for (i in 1:length(perf_table_list)){ #for each file
curr_table <- perf_table_list[[i]]
curr_file <- gsub(perf_dir,"",all_perf_files[[i]])
EachFold_perf_table <- as.data.frame(matrix(NA, nrow =10 ,ncol = 12))
rownames(EachFold_perf_table) <- paste0("Fold",seq(1,10))
colnames(EachFold_perf_table) <-c("AUC","Accuracy","Precision","Sensitivity","Specificity","PPV","NPV",
"Calibration_Intercept","Calibration_Slope",
"Taylor_Calibration_Intercept","Taylor_Calibration_Slope","F1")
for (s in 1:10){ #for each fold
curr_v_tab <- curr_table[which(curr_table[,"TestFold"] == paste0("Fold",s)),]
#calibration
calib_res <- compute_calibration_func(curr_v_tab)
my_calib_res <- calib_res[[1]]
taylors_res <- c(calib_res[[2]],calib_res[[3]])
EachFold_perf_table[s,"Calibration_Intercept"] <- my_calib_res[1]
EachFold_perf_table[s,"Calibration_Slope"] <- my_calib_res[2]
EachFold_perf_table[s,"Taylor_Calibration_Intercept"] <- taylors_res[1]
EachFold_perf_table[s,"Taylor_Calibration_Slope"] <- taylors_res[2]
#Other perforamnce:
other_res <- compute_performance_func(curr_v_tab)
EachFold_perf_table[s,"AUC"] <- as.numeric(other_res[1])
EachFold_perf_table[s,"Accuracy"] <- other_res["Accuracy"]
EachFold_perf_table[s,"Precision"] <- other_res["Precision"]
EachFold_perf_table[s,"Sensitivity"] <- other_res["Sensitivity"]
EachFold_perf_table[s,"Specificity"] <- other_res["Specificity"]
EachFold_perf_table[s,"PPV"] <- other_res["Pos Pred Value"]
EachFold_perf_table[s,"NPV"] <- other_res["Neg Pred Value"]
EachFold_perf_table[s,"F1"] <- other_res["F1"]
}
curr_File_perf <- perf_Mean_CI_Folds_func(EachFold_perf_table)
colnames(curr_File_perf) <- gsub("/|.csv|_pred_table","",curr_file)
File_perf[[i]] <- curr_File_perf
}
Final_perf <- do.call(cbind,File_perf)
write.csv(Final_perf, paste0(outdir , "Final_ALLperf.csv"))
