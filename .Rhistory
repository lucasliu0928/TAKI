AllClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"AllClinicalFeature")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs,SelectedClinicalFeature_perfs2,AllClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6,11,16)]
all_perfs[-c(6,11,16)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
#######################################################################################
##### 3. External Validation Mortality performance
#######################################################################################
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"ExternalV_performance/mortality/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature_15Vars")
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"ExternalV_performance/mortality/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
SOFA_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SOFA")
APACHE_perfs <- get_allmethods_performance(folder_name,perf_file_names,"APACHE")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature15Vars")
all_perfs <- cbind(SOFA_perfs,APACHE_perfs,SelectedClinicalFeature_perfs)
all_perfs <- all_perfs[-c(6,11)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol(all_perfs)))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "SOFA"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
#######################################################################################
##### 4.External Validation  MAKE drop 50
#######################################################################################
rm() #clear all vairbales
perf_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
folder_name <- paste0(perf_dir,"ExternalV_performance/make120_drop50/")
method_names <- c("LogReg","RF","SVM","XGB")
perf_file_names <- paste0("Performance_AVG_CI_",method_names,".csv")
prediction_file_names <- paste0("Prediction_",method_names,".csv")
#1. Performances using different feature
KDIGO_perfs <- get_allmethods_performance(folder_name,perf_file_names,"KDIGO")
SelectedClinicalFeature_perfs <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature14Vars")
SelectedClinicalFeature_perfs2 <- get_allmethods_performance(folder_name,perf_file_names,"SelectedClinicalFeature15Vars")
all_perfs <- cbind(KDIGO_perfs,SelectedClinicalFeature_perfs,SelectedClinicalFeature_perfs2)
all_perfs <- all_perfs[-c(6,11)]
#2.For each featuresets and each method, compare with baseline AUC diff
AUC_diff <- as.data.frame(matrix(NA, nrow = 2, ncol = ncol((all_perfs))))
colnames(AUC_diff) <- colnames(all_perfs)
AUC_diff$Metrics[1] <- "AUC_Diff"
AUC_diff$Metrics[2] <- "AUC_Diff_Pvalue"
baseline_sets <- "KDIGO"
for (i in 2:ncol(AUC_diff)){ #for each feature set
curr_col <- colnames(AUC_diff)[i]
curr_comp_feature <- unlist(strsplit(curr_col,split = "_"))[1]
curr_method <- unlist(strsplit(curr_col,split = "_"))[2]
if (curr_comp_feature != baseline_sets){
#baseline AUC
baseline_auc_colidxes <- which(grepl(paste0(baseline_sets,"_",curr_method),colnames(all_perfs))== T)
baseline_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),baseline_auc_colidxes]
baseline_auc <- as.numeric(unlist(strsplit(baseline_auc,split = "(",fixed = T))[[1]])
tocompare_auc_colidxes <- which(grepl(paste0(curr_comp_feature,"_",curr_method),colnames(all_perfs))== T)
tocompare_auc <-  all_perfs[which(all_perfs$Metrics == "AUC"),tocompare_auc_colidxes]
tocompare_auc <- as.numeric(unlist(strsplit(tocompare_auc,split = "(",fixed = T))[[1]])
AUC_diff[1,i] <- round(tocompare_auc - baseline_auc,2)
baseline_pred_file <- paste0(baseline_sets,"/Prediction_",curr_method,".csv")
tocompare_pred_file <- paste0(curr_comp_feature,"/Prediction_",curr_method,".csv")
p_value <- Test_AUC_diff_func(folder_name,baseline_pred_file,tocompare_pred_file)
if (p_value < 0.001){
p_value <- "< 0.001"
}
AUC_diff[2,i] <- p_value
}else{
AUC_diff[1,i] <- "-"
AUC_diff[2,i] <- "-"
}
}
Final_all_perfs <- rbind(all_perfs,AUC_diff)
reorder_names <- c("AUC" ,"AUC_Diff", "AUC_Diff_Pvalue", "Accuracy" ,"Precision" ,"Sensitivity","Specificity",
"F1",  "PPV" ,"NPV" ,"Calibration_Intercept","Calibration_Slope" ,"Taylor_Calibration_Intercept",
"Taylor_Calibration_Slope")
Final_all_perfs <- Final_all_perfs[match(reorder_names,Final_all_perfs$Metrics),]
write.csv(Final_all_perfs, paste0(folder_name,"Performance_AVG_CI_Altogether.csv"),row.names = F)
source("TAKI_Ultility.R")
compute_avg_pred_risk_and_risk_category <- function(cohort_name,outcome_name,perf_dir,method_name,featureset_folder){
# perf_dir <- UK_mortality_dir
# cohort_name <- "UK"
# outcome_name <- "Mortality"
#1. Load pred table
pred_df <- read.csv(paste0(perf_dir, featureset_folder, "/Prediction_",method_name,".csv"),stringsAsFactors = F)
#2.Compute avg pred risk
avg_risk <- get_avg_pred_func(pred_df)
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
write.csv(risk_count1,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory1_",method_name,".csv"))
risk_category2 <- c(0.2,0.5)
risk_count2 <- count_risk_category(avg_risk,risk_category2)
write.csv(risk_count2,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory2_",method_name,".csv"))
risk_category3 <- c(0.1,0.3,0.5)
risk_count3 <- count_risk_category(avg_risk,risk_category3)
write.csv(risk_count3,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory3_",method_name,".csv"))
}
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
##################################################################################################
######                         Mortality                                            ##############
##################################################################################################
outcome_name <- "Mortality"
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature15Vars"
#1. UK
UK_mortality_dir <- paste0(proj_dir,"CV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_mortality_dir,method_name,featureset_folder)
#2.UTSW
UTSW_mortality_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_mortality_dir,method_name,featureset_folder)
source("TAKI_Ultility.R")
compute_avg_pred_risk_and_risk_category <- function(cohort_name,outcome_name,perf_dir,method_name,featureset_folder){
# perf_dir <- UK_mortality_dir
# cohort_name <- "UK"
# outcome_name <- "Mortality"
#1. Load pred table
pred_df <- read.csv(paste0(perf_dir, featureset_folder, "/Prediction_",method_name,".csv"),stringsAsFactors = F)
#2.Compute avg pred risk
avg_risk <- get_avg_pred_func(pred_df)
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
write.csv(risk_count1,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory1_",method_name,".csv"))
risk_category2 <- c(0.2,0.5)
risk_count2 <- count_risk_category(avg_risk,risk_category2)
write.csv(risk_count2,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory2_",method_name,".csv"))
risk_category3 <- c(0.1,0.3,0.5)
risk_count3 <- count_risk_category(avg_risk,risk_category3)
write.csv(risk_count3,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory3_",method_name,".csv"))
}
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
##################################################################################################
######                         Mortality                                            ##############
##################################################################################################
outcome_name <- "Mortality"
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature15Vars"
#1. UK
UK_mortality_dir <- paste0(proj_dir,"CV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_mortality_dir,method_name,featureset_folder)
#2.UTSW
UTSW_mortality_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_mortality_dir,method_name,featureset_folder)
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature14Vars"
outcome_name <- "MAKE"
#1. UK
UK_MAKE_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_MAKE_dir,method_name,featureset_folder)
#2.UTSW
UTSW_MAKE_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_MAKE_dir,method_name,featureset_folder)
##################################################################################################
############## MAKE ##############
##################################################################################################
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature14Vars"
outcome_name <- "MAKE"
#1. UK
UK_MAKE_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_MAKE_dir,method_name,featureset_folder)
#2.UTSW
UTSW_MAKE_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_MAKE_dir,method_name,featureset_folder)
outcome_name
featureset_folder
perf_dir <- UTSW_MAKE_dir
cohort_name <- "UTSW"
outcome_name <- "MAKE"
method_name <- "RF"
#1. Load pred table
pred_df <- read.csv(paste0(perf_dir, featureset_folder, "/Prediction_",method_name,".csv"),stringsAsFactors = F)
#2.Compute avg pred risk
avg_risk <- get_avg_pred_func(pred_df)
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
avg_risk
quantile(avg_risk$AVG_pred_prob)
risk_category
risk_category <- c(0.1,0.5)
Risk_Count_Table <- as.data.frame(matrix(NA, nrow = length(risk_category) + 1, ncol = 3))
colnames(Risk_Count_Table) <- c("Risk_Category","N_andPerc_PredictedInCategory","N_andPerc_AcutalLabel1")
risk_df
risk_df <- avg_risk
Risk_Count_Table <- as.data.frame(matrix(NA, nrow = length(risk_category) + 1, ncol = 3))
colnames(Risk_Count_Table) <- c("Risk_Category","N_andPerc_PredictedInCategory","N_andPerc_AcutalLabel1")
for (i in 1: (length(risk_category) + 1)){
if (i == 1){
cond <- risk_df[,"AVG_pred_prob"] < risk_category[i]
}else if (i == (length(risk_category) + 1)){
cond <- risk_df[,"AVG_pred_prob"] >= risk_category[i-1]
}else{
cond <- (risk_df[,"AVG_pred_prob"] >= risk_category[i-1]) & (risk_df[,"AVG_pred_prob"] < risk_category[i])
}
curr_pred_df <- risk_df[which(cond == T),]  #Current prediction in this catogry
nPredRisk_in_category <- nrow(curr_pred_df) #n of prediction in this catogry
perc_PredRisk <- round((nPredRisk_in_category/nrow(risk_df)*100),2) # / total n of pts
Risk_Count_Table[i,"N_andPerc_PredictedInCategory"] <- paste0(nPredRisk_in_category," (", perc_PredRisk, ")")
min_risk <- round(min(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual min risk
max_risk <- round(max(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual max risk
Risk_Count_Table[i,"Risk_Category"] <- paste0(min_risk,"% - ", max_risk, "%")
nLabel1_in_category <- length(which(curr_pred_df[,"Label"] == 1)) #number of actual label 1 in current category
perc_label1 <- round(nLabel1_in_category/nPredRisk_in_category*100,2) # / number of predict risk in category
Risk_Count_Table[i,"N_andPerc_AcutalLabel1"] <- paste0(nLabel1_in_category," (", perc_label1, ")")
}
i
cond <- risk_df[,"AVG_pred_prob"] < risk_category[i]
cond
risk_df[,"AVG_pred_prob"]
curr_pred_df <- risk_df[which(cond == T),]  #Current prediction in this catogry
curr_pred_df
nPredRisk_in_category <- nrow(curr_pred_df) #n of prediction in this catogry
nPredRisk_in_category
perc_PredRisk <- round((nPredRisk_in_category/nrow(risk_df)*100),2) # / total n of pts
Risk_Count_Table[i,"N_andPerc_PredictedInCategory"] <- paste0(nPredRisk_in_category," (", perc_PredRisk, ")")
Risk_Count_Table
Risk_Count_Table[i,"Risk_Category"] <- paste0(min_risk,"% - ", max_risk, "%")
min_risk <- round(min(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual min risk
curr_pred_df
Risk_Count_Table <- as.data.frame(matrix(NA, nrow = length(risk_category) + 1, ncol = 3))
colnames(Risk_Count_Table) <- c("Risk_Category","N_andPerc_PredictedInCategory","N_andPerc_AcutalLabel1")
for (i in 1: (length(risk_category) + 1)){
if (i == 1){
cond <- risk_df[,"AVG_pred_prob"] < risk_category[i]
}else if (i == (length(risk_category) + 1)){
cond <- risk_df[,"AVG_pred_prob"] >= risk_category[i-1]
}else{
cond <- (risk_df[,"AVG_pred_prob"] >= risk_category[i-1]) & (risk_df[,"AVG_pred_prob"] < risk_category[i])
}
curr_pred_df <- risk_df[which(cond == T),]  #Current prediction in this catogry
nPredRisk_in_category <- nrow(curr_pred_df) #n of prediction in this catogry
perc_PredRisk <- round((nPredRisk_in_category/nrow(risk_df)*100),2) # / total n of pts
Risk_Count_Table[i,"N_andPerc_PredictedInCategory"] <- paste0(nPredRisk_in_category," (", perc_PredRisk, ")")
if (nrow(curr_pred_df) == 0){
min_risk <- NA
max_risk <- NA
}else{
min_risk <- round(min(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual min risk
max_risk <- round(max(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual max risk
}
Risk_Count_Table[i,"Risk_Category"] <- paste0(min_risk,"% - ", max_risk, "%")
nLabel1_in_category <- length(which(curr_pred_df[,"Label"] == 1)) #number of actual label 1 in current category
perc_label1 <- round(nLabel1_in_category/nPredRisk_in_category*100,2) # / number of predict risk in category
Risk_Count_Table[i,"N_andPerc_AcutalLabel1"] <- paste0(nLabel1_in_category," (", perc_label1, ")")
}
Risk_Count_Table
quantile(risk_df$AVG_pred_prob)
risk_category
1357+876
perc_PredRisk
Risk_Count_Table
risk_category <- c(0.1,0.5)
risk_df <- avg_risk
Risk_Count_Table <- as.data.frame(matrix(NA, nrow = length(risk_category) + 1, ncol = 3))
colnames(Risk_Count_Table) <- c("Risk_Category","N_andPerc_PredictedInCategory","N_andPerc_AcutalLabel1")
for (i in 1: (length(risk_category) + 1)){
if (i == 1){
cond <- risk_df[,"AVG_pred_prob"] < risk_category[i]
}else if (i == (length(risk_category) + 1)){
cond <- risk_df[,"AVG_pred_prob"] >= risk_category[i-1]
}else{
cond <- (risk_df[,"AVG_pred_prob"] >= risk_category[i-1]) & (risk_df[,"AVG_pred_prob"] < risk_category[i])
}
curr_pred_df <- risk_df[which(cond == T),]  #Current prediction in this catogry
nPredRisk_in_category <- nrow(curr_pred_df) #n of prediction in this catogry
perc_PredRisk <- round((nPredRisk_in_category/nrow(risk_df)*100),2) # / total n of pts
Risk_Count_Table[i,"N_andPerc_PredictedInCategory"] <- paste0(nPredRisk_in_category," (", perc_PredRisk, ")")
if (nrow(curr_pred_df) == 0){
Risk_Count_Table[i,"Risk_Category"] <- "NONE"
}else{
min_risk <- round(min(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual min risk
max_risk <- round(max(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual max risk
Risk_Count_Table[i,"Risk_Category"] <- paste0(min_risk,"% - ", max_risk, "%")
}
nLabel1_in_category <- length(which(curr_pred_df[,"Label"] == 1)) #number of actual label 1 in current category
perc_label1 <- round(nLabel1_in_category/nPredRisk_in_category*100,2) # / number of predict risk in category
Risk_Count_Table[i,"N_andPerc_AcutalLabel1"] <- paste0(nLabel1_in_category," (", perc_label1, ")")
}
Risk_Count_Table
count_risk_category <- function(risk_df,risk_category){
# risk_category <- c(0.1,0.5)
# risk_df <- avg_risk
#
Risk_Count_Table <- as.data.frame(matrix(NA, nrow = length(risk_category) + 1, ncol = 3))
colnames(Risk_Count_Table) <- c("Risk_Category","N_andPerc_PredictedInCategory","N_andPerc_AcutalLabel1")
for (i in 1: (length(risk_category) + 1)){
if (i == 1){
cond <- risk_df[,"AVG_pred_prob"] < risk_category[i]
}else if (i == (length(risk_category) + 1)){
cond <- risk_df[,"AVG_pred_prob"] >= risk_category[i-1]
}else{
cond <- (risk_df[,"AVG_pred_prob"] >= risk_category[i-1]) & (risk_df[,"AVG_pred_prob"] < risk_category[i])
}
curr_pred_df <- risk_df[which(cond == T),]  #Current prediction in this catogry
nPredRisk_in_category <- nrow(curr_pred_df) #n of prediction in this catogry
perc_PredRisk <- round((nPredRisk_in_category/nrow(risk_df)*100),2) # / total n of pts
Risk_Count_Table[i,"N_andPerc_PredictedInCategory"] <- paste0(nPredRisk_in_category," (", perc_PredRisk, ")")
if (nrow(curr_pred_df) == 0){
Risk_Count_Table[i,"Risk_Category"] <- "NONE"
Risk_Count_Table[i,"N_andPerc_AcutalLabel1"] <- paste0(0," (", 0, ")")
}else{
min_risk <- round(min(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual min risk
max_risk <- round(max(curr_pred_df[,"AVG_pred_prob"]),4)*100 #actual max risk
Risk_Count_Table[i,"Risk_Category"] <- paste0(min_risk,"% - ", max_risk, "%")
nLabel1_in_category <- length(which(curr_pred_df[,"Label"] == 1)) #number of actual label 1 in current category
perc_label1 <- round(nLabel1_in_category/nPredRisk_in_category*100,2) # / number of predict risk in category
Risk_Count_Table[i,"N_andPerc_AcutalLabel1"] <- paste0(nLabel1_in_category," (", perc_label1, ")")
}
}
return(Risk_Count_Table)
}
compute_avg_pred_risk_and_risk_category <- function(cohort_name,outcome_name,perf_dir,method_name,featureset_folder){
# perf_dir <- UTSW_MAKE_dir
# cohort_name <- "UTSW"
# outcome_name <- "MAKE"
# method_name <- "RF"
#1. Load pred table
pred_df <- read.csv(paste0(perf_dir, featureset_folder, "/Prediction_",method_name,".csv"),stringsAsFactors = F)
#2.Compute avg pred risk
avg_risk <- get_avg_pred_func(pred_df)
write.csv(avg_risk,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_AVG_Pred_Risk_",method_name,".csv"))
#3.Count risk category
risk_category1 <- c(0.1,0.5)
risk_count1 <- count_risk_category(avg_risk,risk_category1)
write.csv(risk_count1,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory1_",method_name,".csv"))
risk_category2 <- c(0.2,0.5)
risk_count2 <- count_risk_category(avg_risk,risk_category2)
write.csv(risk_count2,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory2_",method_name,".csv"))
risk_category3 <- c(0.1,0.3,0.5)
risk_count3 <- count_risk_category(avg_risk,risk_category3)
write.csv(risk_count3,paste0(perf_dir,cohort_name,"_",featureset_folder,"_",outcome_name,"_Risk_Catogory3_",method_name,".csv"))
}
#1. UK
method_name <- "RF"
featureset_folder <- "SelectedClinicalFeature14Vars"
outcome_name <- "MAKE"
#1. UK
UK_MAKE_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UK",outcome_name,UK_MAKE_dir,method_name,featureset_folder)
#2.UTSW
UTSW_MAKE_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
compute_avg_pred_risk_and_risk_category("UTSW",outcome_name,UTSW_MAKE_dir,method_name,featureset_folder)
source('~/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/TAKI_Code/3D_Get_Avg_PredictionRisk.R', echo=TRUE)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature2vsSOFA_",colnames(reclass_res2)[2])
comb_res <- cbind(reclass_res1,reclass_res2)
write.csv(comb_res,paste0(perf_dir,"UK_SelectedClinicalFeature2_Mortality_ReclassResults_RF.csv"))
#2. UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature2vsSOFA_",colnames(reclass_res2)[2])
comb_res <- cbind(reclass_res1,reclass_res2)
write.csv(comb_res,paste0(perf_dir,"UTSW_SelectedClinicalFeature2_Mortality_ReclassResults_RF.csv"))
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0708/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
source('~/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/TAKI_Code/3E_Compute_Reclass_Stats.R', echo=TRUE)
source("TAKI_Ultility.R")
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/Intermediate_Results/Prediction_results0806/"
###############################################################
#1. For mortality, compare models with IDI, NRI
###############################################################
#1. For UK
perf_dir <- paste0(proj_dir,"CV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
comb_res <- cbind(reclass_res1,reclass_res2)
write.csv(comb_res,paste0(perf_dir,"UK_SelectedClinicalFeature15Vars_Mortality_ReclassResults_RF.csv"))
#2. UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/mortality/")
baseline_model_file  <- "/SOFA/Prediction_RF.csv"
comprison_model_file1 <- "/APACHE/Prediction_RF.csv"
comprison_model_file2 <- "/SelectedClinicalFeature15Vars/Prediction_RF.csv"
reclass_res1 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res1)[2] <- paste0("APACHEvsSOFA_",colnames(reclass_res1)[2])
reclass_res2 <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file2,cutoff = c(0,0.5,1))
colnames(reclass_res2)[2] <- paste0("SelectedClinicalFeature15VarsvsSOFA_",colnames(reclass_res2)[2])
comb_res <- cbind(reclass_res1,reclass_res2)
write.csv(comb_res,paste0(perf_dir,"UTSW_SelectedClinicalFeature15Vars_Mortality_ReclassResults_RF.csv"))
#1.UK
perf_dir <- paste0(proj_dir,"CV_performance/make120_drop50/")
baseline_model_file  <- "/KDIGO/Prediction_RF.csv"
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
write.csv(reclass_res,paste0(perf_dir,"UK_SelectedClinicalFeature14Vars_MAKE_ReclassResults_RF.csv"))
#UTSW
perf_dir <- paste0(proj_dir,"ExternalV_performance/make120_drop50/")
baseline_model_file  <- "/KDIGO/Prediction_RF.csv"
comprison_model_file1 <- "/SelectedClinicalFeature14Vars/Prediction_RF.csv"
reclass_res <- compute_IDI_NRI_func(perf_dir,baseline_model_file,comprison_model_file1,cutoff = c(0,0.5,1))
colnames(reclass_res)[2] <- paste0("SelectedClinicalFeature14VarsvsKDIGO_",colnames(reclass_res)[2])
write.csv(reclass_res,paste0(perf_dir,"UTSW_SelectedClinicalFeature14Varsl_MAKE_ReclassResults_RF.csv"))
source('~/Desktop/DrChen_Projects/All_AKI_Projects/Other_Project/TAKI_Project/TAKI_Code/4B_Compute_DiscripStats.R', echo=TRUE)
